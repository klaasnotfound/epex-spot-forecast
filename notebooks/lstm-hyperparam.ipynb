{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather and spot market data from the DB\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "data_start = \"2023-01-01\"\n",
    "data_end = \"2025-07-01\"\n",
    "\n",
    "db_filepath = \"data/db/local.db\"\n",
    "con = duckdb.connect(db_filepath)\n",
    "weather_cols = [\n",
    "    \"temperature_2m_degc\",\n",
    "    \"shortwave_radiation_wm2\",\n",
    "    \"direct_radiation_wm2\",\n",
    "    \"diffuse_radiation_wm2\",\n",
    "    \"direct_normal_irradiance_wm2\",\n",
    "    \"global_tilted_irradiance_wm2\",\n",
    "    \"terrestrial_radiation_wm2\",\n",
    "    \"wind_speed_10m_kmh\",\n",
    "    \"wind_speed_80m_kmh\",\n",
    "    \"wind_speed_120m_kmh\",\n",
    "    \"cloud_cover_perc\",\n",
    "    \"cloud_cover_low_perc\",\n",
    "    \"cloud_cover_mid_perc\",\n",
    "    \"cloud_cover_high_perc\",\n",
    "    \"visibility_m\",\n",
    "]\n",
    "w_cols = \", \".join([f\"open_meteo_agg_hourly.{col}\" for col in weather_cols])\n",
    "market_cols = [\n",
    "    \"non_ren_prod_kw\",\n",
    "    \"ren_prod_kw\",\n",
    "    \"load_kw\",\n",
    "    \"daa_price_eurmwh\",\n",
    "    \"idc_av_price_eurmwh\",\n",
    "    \"idc_low_price_eurmwh\",\n",
    "    \"idc_high_price_eurmwh\",\n",
    "]\n",
    "m_cols = \", \".join([f\"epex_market.{col}\" for col in market_cols])\n",
    "\n",
    "data = con.sql(f\"\"\"\n",
    "              SELECT open_meteo_agg_hourly.ts, {w_cols}, {m_cols}\n",
    "              FROM open_meteo_agg_hourly\n",
    "              JOIN epex_market ON open_meteo_agg_hourly.ts = epex_market.ts\n",
    "              WHERE open_meteo_agg_hourly.ts >= '{data_start}'\n",
    "                AND open_meteo_agg_hourly.ts < '{data_end}'\n",
    "              ORDER BY open_meteo_agg_hourly.ts\n",
    "              \"\"\").df()\n",
    "data.index = pd.Index(data[\"ts\"])\n",
    "data = data.drop(\"ts\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392256dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and create sets / loaders\n",
    "\n",
    "# Copy the original\n",
    "tdata = data.copy()\n",
    "\n",
    "# We can only use EPEX values from the previous day\n",
    "last_weather_col_idx = tdata.columns.tolist().index(weather_cols[-1])\n",
    "first_weather_col_idx = tdata.columns.tolist().index(weather_cols[0])\n",
    "today_cols = tdata.columns[0:last_weather_col_idx]\n",
    "tdata[today_cols] = tdata[today_cols].shift(-24)\n",
    "tdata = tdata[:-24]\n",
    "\n",
    "# Fill N/A values\n",
    "na_columns = tdata.columns[tdata.isna().any()].tolist()\n",
    "for col in na_columns:\n",
    "    tdata[col] = tdata[col].fillna(tdata[col].mean())\n",
    "\n",
    "# Fill in missing daylight-saving hours\n",
    "for idx in range(1, tdata.shape[0]):\n",
    "    ts = tdata.index[idx]\n",
    "    prev_ts = tdata.index[idx - 1]\n",
    "    diff = (ts.hour - prev_ts.hour) % 24\n",
    "    if diff != 1:\n",
    "        iso_str = f\"{ts.year}-{ts.month:02d}-{ts.day:02d}T{(ts.hour - 1):02d}:00:00\"\n",
    "        new_ts = pd.to_datetime(iso_str)\n",
    "        tdata = pd.concat(\n",
    "            [\n",
    "                tdata,\n",
    "                pd.DataFrame(tdata.loc[prev_ts].to_dict(), index=[new_ts]),\n",
    "            ]\n",
    "        )\n",
    "tdata.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "# Note down the price amplitude before normalizing\n",
    "price_mean = tdata[\"idc_av_price_eurmwh\"].mean()\n",
    "price_std = tdata[\"idc_av_price_eurmwh\"].std()\n",
    "\n",
    "# Normalize with min/max for the fixed date columns, mean for the rest\n",
    "for c in tdata.columns[1:first_weather_col_idx]:\n",
    "    tdata[c] = (tdata[c] - tdata[c].min()) / (tdata[c].max() - tdata[c].min())\n",
    "tdata[tdata.columns[first_weather_col_idx:]] = (\n",
    "    tdata[tdata.columns[first_weather_col_idx:]]\n",
    "    - tdata[tdata.columns[first_weather_col_idx:]].mean()\n",
    ") / tdata[tdata.columns[first_weather_col_idx:]].std()\n",
    "\n",
    "# Create training and test rows\n",
    "split_idx = int((0.8 * len(tdata)) // 24 * 24)\n",
    "iso_str = f\"{tdata.index[split_idx]}\"[:10]\n",
    "cutoff_date = pd.to_datetime(iso_str)\n",
    "train_data = tdata.loc[(tdata.index < cutoff_date)]\n",
    "test_data = tdata.loc[(tdata.index >= cutoff_date)]\n",
    "train_rows = train_data.values.astype(\"float32\")\n",
    "test_rows = test_data.values.astype(\"float32\")\n",
    "price_idx = train_data.columns.tolist().index(\"idc_av_price_eurmwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8919c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/gxps7b2d6l3dps4vtczb2rh80000gn/T/ipykernel_47123/1157537686.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  return torch.tensor(X), torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and loaders\n",
    "\n",
    "from numpy import ndarray\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Datasets are created from sliding windows\n",
    "def create_dataset(\n",
    "    data: ndarray, lookback_hours: int, predict_hours: int\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    X, y = [], []\n",
    "    for i in range(lookback_hours, len(data) - predict_hours, 24):\n",
    "        feature = data[i - lookback_hours : i]\n",
    "        target = data[i : i + predict_hours][:, price_idx]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "# Create the sets\n",
    "lookback_hours = 1 * 24\n",
    "predict_hours = 1 * 24\n",
    "X_train, y_train = create_dataset(train_rows, lookback_hours, predict_hours)\n",
    "X_test, y_test = create_dataset(test_rows, lookback_hours, predict_hours)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe71d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size: int, num_layers=2, hidden_size=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Linear(hidden_size // 2, 8),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9ffc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Layers\tHidden Size\t# Params\tBlend Factor\tVal. Loss\tØ Diff DAA (EUR)\tσ Diff DAA (EUR)\tØ Diff Pred. (EUR)\tσ Diff Pred. (EUR)\tØ Diff Blended (EUR)\tσ Diff Blended (EUR)\n",
      "2\t64\t58161\t0.2\t0.2218\t12.781\t33.986\t20.682\t21.148\t12.110\t13.525\n",
      "2\t64\t58161\t0.2\t0.2444\t12.781\t33.986\t20.688\t20.679\t12.032\t13.471\n",
      "2\t64\t58161\t0.2\t0.2334\t12.781\t33.986\t20.411\t21.131\t11.981\t13.406\n",
      "2\t64\t58161\t0.2\t0.2395\t12.781\t33.986\t21.244\t21.212\t11.989\t13.504\n",
      "2\t64\t58161\t0.2\t0.2316\t12.781\t33.986\t20.632\t20.680\t12.054\t13.456\n",
      "3\t64\t91441\t0.2\t0.2390\t12.781\t33.986\t20.707\t20.627\t11.899\t13.408\n",
      "3\t64\t91441\t0.2\t0.2360\t12.781\t33.986\t20.887\t20.661\t11.905\t13.425\n",
      "3\t64\t91441\t0.2\t0.2355\t12.781\t33.986\t21.003\t21.207\t11.812\t13.296\n",
      "3\t64\t91441\t0.2\t0.2375\t12.781\t33.986\t21.076\t20.760\t11.935\t13.417\n",
      "3\t64\t91441\t0.2\t0.2320\t12.781\t33.986\t20.583\t22.267\t11.977\t13.665\n",
      "4\t64\t124721\t0.2\t0.2287\t12.781\t33.986\t20.614\t20.892\t12.210\t13.474\n",
      "4\t64\t124721\t0.2\t0.2458\t12.781\t33.986\t20.773\t21.329\t11.867\t13.425\n",
      "4\t64\t124721\t0.2\t0.2420\t12.781\t33.986\t20.728\t21.180\t12.008\t13.268\n",
      "4\t64\t124721\t0.2\t0.2437\t12.781\t33.986\t21.009\t21.483\t12.131\t13.350\n",
      "4\t64\t124721\t0.2\t0.2403\t12.781\t33.986\t20.987\t21.993\t12.018\t13.429\n",
      "2\t96\t125633\t0.2\t0.2438\t12.781\t33.986\t20.522\t21.024\t11.776\t13.525\n",
      "2\t96\t125633\t0.2\t0.2368\t12.781\t33.986\t21.159\t20.589\t12.055\t13.365\n",
      "2\t96\t125633\t0.2\t0.2315\t12.781\t33.986\t21.660\t21.180\t11.942\t13.183\n",
      "2\t96\t125633\t0.2\t0.2312\t12.781\t33.986\t20.800\t20.329\t11.949\t13.381\n",
      "2\t96\t125633\t0.2\t0.2324\t12.781\t33.986\t20.796\t21.249\t11.982\t13.441\n",
      "3\t96\t200129\t0.2\t0.2419\t12.781\t33.986\t20.797\t20.858\t11.953\t13.283\n",
      "3\t96\t200129\t0.2\t0.2310\t12.781\t33.986\t21.329\t20.780\t11.954\t13.458\n",
      "3\t96\t200129\t0.2\t0.2320\t12.781\t33.986\t20.779\t20.631\t11.873\t13.363\n",
      "3\t96\t200129\t0.2\t0.2268\t12.781\t33.986\t20.554\t20.568\t12.026\t13.565\n",
      "3\t96\t200129\t0.2\t0.2301\t12.781\t33.986\t20.487\t20.206\t12.040\t13.389\n",
      "4\t96\t274625\t0.2\t0.2370\t12.781\t33.986\t20.889\t20.935\t12.192\t13.513\n",
      "4\t96\t274625\t0.2\t0.2418\t12.781\t33.986\t21.945\t22.506\t12.155\t13.367\n",
      "4\t96\t274625\t0.2\t0.2281\t12.781\t33.986\t20.090\t21.344\t11.873\t13.415\n",
      "4\t96\t274625\t0.2\t0.2266\t12.781\t33.986\t21.311\t22.134\t12.027\t13.438\n",
      "4\t96\t274625\t0.2\t0.2291\t12.781\t33.986\t20.687\t21.546\t12.033\t13.693\n",
      "2\t128\t218705\t0.2\t0.2343\t12.781\t33.986\t20.539\t21.191\t11.960\t13.327\n",
      "2\t128\t218705\t0.2\t0.2323\t12.781\t33.986\t20.952\t20.376\t12.090\t13.425\n",
      "2\t128\t218705\t0.2\t0.2337\t12.781\t33.986\t21.130\t20.880\t12.180\t13.456\n",
      "2\t128\t218705\t0.2\t0.2365\t12.781\t33.986\t21.187\t21.463\t12.074\t13.446\n",
      "2\t128\t218705\t0.2\t0.2435\t12.781\t33.986\t20.831\t20.852\t12.028\t13.380\n",
      "3\t128\t350801\t0.2\t0.2374\t12.781\t33.986\t21.677\t21.715\t12.239\t13.347\n",
      "3\t128\t350801\t0.2\t0.2453\t12.781\t33.986\t20.988\t21.501\t11.840\t13.484\n",
      "3\t128\t350801\t0.2\t0.2308\t12.781\t33.986\t21.904\t22.073\t12.227\t13.287\n",
      "3\t128\t350801\t0.2\t0.2378\t12.781\t33.986\t20.794\t20.126\t12.084\t13.351\n",
      "3\t128\t350801\t0.2\t0.2322\t12.781\t33.986\t20.478\t20.398\t12.128\t13.456\n",
      "4\t128\t482897\t0.2\t0.2380\t12.781\t33.986\t21.046\t21.566\t12.021\t13.334\n",
      "4\t128\t482897\t0.2\t0.2284\t12.781\t33.986\t20.453\t20.978\t12.123\t13.382\n",
      "4\t128\t482897\t0.2\t0.2345\t12.781\t33.986\t20.541\t21.147\t11.974\t13.501\n",
      "4\t128\t482897\t0.2\t0.2387\t12.781\t33.986\t20.946\t21.077\t11.944\t13.498\n",
      "4\t128\t482897\t0.2\t0.2267\t12.781\t33.986\t19.682\t20.698\t11.942\t13.460\n",
      "2\t192\t481649\t0.2\t0.2180\t12.781\t33.986\t20.769\t20.968\t12.119\t13.362\n",
      "2\t192\t481649\t0.2\t0.2306\t12.781\t33.986\t20.648\t20.416\t11.933\t13.339\n",
      "2\t192\t481649\t0.2\t0.2345\t12.781\t33.986\t21.572\t21.836\t12.148\t13.484\n",
      "2\t192\t481649\t0.2\t0.2297\t12.781\t33.986\t19.893\t20.326\t11.880\t13.425\n",
      "2\t192\t481649\t0.2\t0.2346\t12.781\t33.986\t21.030\t21.762\t11.958\t13.420\n",
      "3\t192\t778097\t0.2\t0.2347\t12.781\t33.986\t20.353\t20.973\t11.863\t13.391\n",
      "3\t192\t778097\t0.2\t0.2440\t12.781\t33.986\t21.802\t22.370\t12.092\t13.559\n",
      "3\t192\t778097\t0.2\t0.2307\t12.781\t33.986\t20.939\t20.677\t12.100\t13.379\n",
      "3\t192\t778097\t0.2\t0.2357\t12.781\t33.986\t21.104\t22.037\t12.079\t13.577\n",
      "3\t192\t778097\t0.2\t0.2291\t12.781\t33.986\t21.074\t21.737\t12.065\t13.280\n",
      "4\t192\t1074545\t0.2\t0.2279\t12.781\t33.986\t21.538\t22.502\t12.047\t13.453\n",
      "4\t192\t1074545\t0.2\t0.2290\t12.781\t33.986\t20.619\t20.671\t11.926\t13.349\n",
      "4\t192\t1074545\t0.2\t0.2292\t12.781\t33.986\t21.173\t21.630\t12.144\t13.389\n",
      "4\t192\t1074545\t0.2\t0.2323\t12.781\t33.986\t21.613\t21.767\t12.131\t13.490\n",
      "4\t192\t1074545\t0.2\t0.2356\t12.781\t33.986\t21.947\t21.295\t12.165\t13.550\n",
      "2\t256\t846993\t0.2\t0.2256\t12.781\t33.986\t20.281\t21.050\t11.990\t13.352\n",
      "2\t256\t846993\t0.2\t0.2398\t12.781\t33.986\t20.445\t21.346\t11.925\t13.511\n",
      "2\t256\t846993\t0.2\t0.2267\t12.781\t33.986\t20.822\t20.913\t12.073\t13.345\n",
      "2\t256\t846993\t0.2\t0.2361\t12.781\t33.986\t21.261\t21.433\t12.058\t13.300\n",
      "2\t256\t846993\t0.2\t0.2288\t12.781\t33.986\t20.538\t19.954\t11.914\t13.464\n",
      "3\t256\t1373329\t0.2\t0.2359\t12.781\t33.986\t22.503\t22.304\t12.134\t13.466\n",
      "3\t256\t1373329\t0.2\t0.2376\t12.781\t33.986\t21.076\t21.152\t12.092\t13.347\n",
      "3\t256\t1373329\t0.2\t0.2305\t12.781\t33.986\t20.887\t21.018\t11.882\t13.319\n",
      "3\t256\t1373329\t0.2\t0.2415\t12.781\t33.986\t21.094\t21.050\t12.055\t13.518\n",
      "3\t256\t1373329\t0.2\t0.2258\t12.781\t33.986\t20.721\t21.360\t11.882\t13.294\n",
      "4\t256\t1899665\t0.2\t0.2316\t12.781\t33.986\t22.246\t22.318\t12.307\t13.516\n",
      "4\t256\t1899665\t0.2\t0.2362\t12.781\t33.986\t20.349\t21.512\t11.931\t13.312\n",
      "4\t256\t1899665\t0.2\t0.2334\t12.781\t33.986\t21.672\t22.024\t12.240\t13.355\n",
      "4\t256\t1899665\t0.2\t0.2405\t12.781\t33.986\t20.890\t21.137\t12.029\t13.317\n",
      "4\t256\t1899665\t0.2\t0.2345\t12.781\t33.986\t21.835\t21.373\t11.971\t13.421\n",
      "2\t384\t1884881\t0.2\t0.2277\t12.781\t33.986\t20.110\t20.037\t11.904\t13.327\n",
      "2\t384\t1884881\t0.2\t0.2317\t12.781\t33.986\t20.491\t20.330\t11.891\t13.333\n",
      "2\t384\t1884881\t0.2\t0.2376\t12.781\t33.986\t21.253\t21.034\t12.104\t13.282\n",
      "2\t384\t1884881\t0.2\t0.2348\t12.781\t33.986\t20.591\t20.993\t11.952\t13.290\n",
      "2\t384\t1884881\t0.2\t0.2355\t12.781\t33.986\t21.027\t20.723\t12.129\t13.361\n",
      "3\t384\t3067601\t0.2\t0.2458\t12.781\t33.986\t20.940\t20.946\t12.114\t13.323\n",
      "3\t384\t3067601\t0.2\t0.2304\t12.781\t33.986\t21.023\t22.116\t12.051\t13.478\n",
      "3\t384\t3067601\t0.2\t0.2384\t12.781\t33.986\t21.925\t21.542\t12.263\t13.446\n",
      "3\t384\t3067601\t0.2\t0.2351\t12.781\t33.986\t21.697\t21.900\t12.176\t13.414\n",
      "3\t384\t3067601\t0.2\t0.2350\t12.781\t33.986\t20.531\t20.594\t11.928\t13.421\n",
      "4\t384\t4250321\t0.2\t0.2276\t12.781\t33.986\t20.658\t21.436\t11.978\t13.331\n",
      "4\t384\t4250321\t0.2\t0.2316\t12.781\t33.986\t20.954\t20.160\t12.077\t13.324\n",
      "4\t384\t4250321\t0.2\t0.2329\t12.781\t33.986\t21.732\t22.783\t12.212\t13.504\n",
      "4\t384\t4250321\t0.2\t0.2376\t12.781\t33.986\t22.517\t23.897\t12.202\t13.412\n",
      "4\t384\t4250321\t0.2\t0.2290\t12.781\t33.986\t21.312\t20.580\t11.996\t13.507\n"
     ]
    }
   ],
   "source": [
    "# Iterate hyperparams, train and evaluate the model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "max_epochs = 200\n",
    "patience = 10\n",
    "blend_fac = 0.2\n",
    "\n",
    "# Print the header\n",
    "print(\n",
    "    \"# Layers\\tHidden Size\\t# Params\\tBlend Factor\\tVal. Loss\\tØ Diff DAA (EUR)\\tσ Diff DAA (EUR)\\tØ Diff Pred. (EUR)\\tσ Diff Pred. (EUR)\\tØ Diff Blended (EUR)\\tσ Diff Blended (EUR)\"\n",
    ")\n",
    "\n",
    "for hidden_size in [64, 96, 128, 192, 256, 384]:\n",
    "    for num_layers in [2, 3, 4]:\n",
    "        for run in range(5):\n",
    "            # Create the model\n",
    "            loss_fn = nn.MSELoss()\n",
    "            model = LSTMModel(X_train.shape[2], num_layers, hidden_size).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"min\", factor=0.5, patience=5\n",
    "            )\n",
    "            trainable_params = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            )\n",
    "\n",
    "            # Train the model until there's no more improvement\n",
    "            best_val_loss = float(\"inf\")\n",
    "            best_weights = None\n",
    "            patience_counter = 0\n",
    "            for epoch in range(max_epochs):\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(X_batch)\n",
    "                    loss = loss_fn(output, torch.unsqueeze(y_batch, 2))\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                # Calculate validation loss\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in test_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        output = model(torch.squeeze(X_batch))\n",
    "                        val_loss += loss_fn(output, torch.unsqueeze(y_batch, 2)).item()\n",
    "                val_loss /= len(test_loader)\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "                # Stop early if there's no more improvement\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_weights = model.state_dict()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        break\n",
    "\n",
    "            # Evaluate the price difference\n",
    "            with torch.no_grad():\n",
    "                model.cpu()\n",
    "                # Get the predicted data\n",
    "                y_pred_test = model(X_test).clone().detach().cpu().numpy()\n",
    "                y_pred_test = y_pred_test.reshape(\n",
    "                    y_pred_test.shape[0] * y_pred_test.shape[1]\n",
    "                )\n",
    "                test_plot = np.ones_like(tdata[\"idc_av_price_eurmwh\"]) * np.nan\n",
    "                test_plot[\n",
    "                    len(train_rows) + lookback_hours : len(train_rows)\n",
    "                    + lookback_hours\n",
    "                    + y_pred_test.shape[0]\n",
    "                ] = y_pred_test\n",
    "\n",
    "                # Transform it back\n",
    "                real_price = (\n",
    "                    tdata[\"idc_av_price_eurmwh\"].to_numpy() * price_std + price_mean\n",
    "                )\n",
    "                daa_price = (\n",
    "                    tdata[\"daa_price_eurmwh\"].to_numpy() * price_std + price_mean\n",
    "                )\n",
    "                pred_test_price = test_plot * price_std + price_mean\n",
    "\n",
    "                # Calculate the blended price\n",
    "                blend_price = (1 - blend_fac) * daa_price + blend_fac * pred_test_price\n",
    "\n",
    "                # Calculate the absolute diff mean and stddev\n",
    "                pdata = pd.DataFrame({\"intraday_price\": real_price}, index=tdata.index)\n",
    "                daa_diff = pdata[\"intraday_price\"] - daa_price\n",
    "                daa_adm = daa_diff.abs().mean()\n",
    "                daa_ads = daa_diff.abs().std()\n",
    "                pred_diff = pdata[\"intraday_price\"] - pred_test_price\n",
    "                pred_adm = pred_diff.abs().mean()\n",
    "                pred_ads = pred_diff.abs().std()\n",
    "                blend_diff = pdata[\"intraday_price\"] - blend_price\n",
    "                blend_adm = blend_diff.abs().mean()\n",
    "                blend_ads = blend_diff.abs().std()\n",
    "\n",
    "                # Print the result\n",
    "                print(\n",
    "                    f\"{num_layers}\\t{hidden_size}\\t{trainable_params}\\t{blend_fac}\\t{best_val_loss:.4f}\\t{daa_adm:.3f}\\t{daa_ads:.3f}\\t{pred_adm:.3f}\\t{pred_ads:.3f}\\t{blend_adm:.3f}\\t{blend_ads:.3f}\"\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
