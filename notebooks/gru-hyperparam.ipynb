{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather and spot market data from the DB\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "data_start = \"2023-01-01\"\n",
    "data_end = \"2025-07-01\"\n",
    "\n",
    "db_filepath = \"data/db/local.db\"\n",
    "con = duckdb.connect(db_filepath)\n",
    "weather_cols = [\n",
    "    \"temperature_2m_degc\",\n",
    "    \"shortwave_radiation_wm2\",\n",
    "    \"direct_radiation_wm2\",\n",
    "    \"diffuse_radiation_wm2\",\n",
    "    \"direct_normal_irradiance_wm2\",\n",
    "    \"global_tilted_irradiance_wm2\",\n",
    "    \"terrestrial_radiation_wm2\",\n",
    "    \"wind_speed_10m_kmh\",\n",
    "    \"wind_speed_80m_kmh\",\n",
    "    \"wind_speed_120m_kmh\",\n",
    "    \"cloud_cover_perc\",\n",
    "    \"cloud_cover_low_perc\",\n",
    "    \"cloud_cover_mid_perc\",\n",
    "    \"cloud_cover_high_perc\",\n",
    "    \"visibility_m\",\n",
    "]\n",
    "w_cols = \", \".join([f\"open_meteo_agg_hourly.{col}\" for col in weather_cols])\n",
    "market_cols = [\n",
    "    \"non_ren_prod_kw\",\n",
    "    \"ren_prod_kw\",\n",
    "    \"load_kw\",\n",
    "    \"daa_price_eurmwh\",\n",
    "    \"idc_av_price_eurmwh\",\n",
    "    \"idc_low_price_eurmwh\",\n",
    "    \"idc_high_price_eurmwh\",\n",
    "]\n",
    "m_cols = \", \".join([f\"epex_market.{col}\" for col in market_cols])\n",
    "\n",
    "data = con.sql(f\"\"\"\n",
    "              SELECT open_meteo_agg_hourly.ts, {w_cols}, {m_cols}\n",
    "              FROM open_meteo_agg_hourly\n",
    "              JOIN epex_market ON open_meteo_agg_hourly.ts = epex_market.ts\n",
    "              WHERE open_meteo_agg_hourly.ts >= '{data_start}'\n",
    "                AND open_meteo_agg_hourly.ts < '{data_end}'\n",
    "              ORDER BY open_meteo_agg_hourly.ts\n",
    "              \"\"\").df()\n",
    "data.index = pd.Index(data[\"ts\"])\n",
    "data = data.drop(\"ts\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392256dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and create sets / loaders\n",
    "\n",
    "# Copy the original\n",
    "tdata = data.copy()\n",
    "\n",
    "# We can only use EPEX values from the previous day\n",
    "last_weather_col_idx = tdata.columns.tolist().index(weather_cols[-1])\n",
    "first_weather_col_idx = tdata.columns.tolist().index(weather_cols[0])\n",
    "today_cols = tdata.columns[0:last_weather_col_idx]\n",
    "tdata[today_cols] = tdata[today_cols].shift(-24)\n",
    "tdata = tdata[:-24]\n",
    "\n",
    "# Fill N/A values\n",
    "na_columns = tdata.columns[tdata.isna().any()].tolist()\n",
    "for col in na_columns:\n",
    "    tdata[col] = tdata[col].fillna(tdata[col].mean())\n",
    "\n",
    "# Fill in missing daylight-saving hours\n",
    "for idx in range(1, tdata.shape[0]):\n",
    "    ts = tdata.index[idx]\n",
    "    prev_ts = tdata.index[idx - 1]\n",
    "    diff = (ts.hour - prev_ts.hour) % 24\n",
    "    if diff != 1:\n",
    "        iso_str = f\"{ts.year}-{ts.month:02d}-{ts.day:02d}T{(ts.hour - 1):02d}:00:00\"\n",
    "        new_ts = pd.to_datetime(iso_str)\n",
    "        tdata = pd.concat(\n",
    "            [\n",
    "                tdata,\n",
    "                pd.DataFrame(tdata.loc[prev_ts].to_dict(), index=[new_ts]),\n",
    "            ]\n",
    "        )\n",
    "tdata.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "# Note down the price amplitude before normalizing\n",
    "price_mean = tdata[\"idc_av_price_eurmwh\"].mean()\n",
    "price_std = tdata[\"idc_av_price_eurmwh\"].std()\n",
    "\n",
    "# Normalize with min/max for the fixed date columns, mean for the rest\n",
    "for c in tdata.columns[1:first_weather_col_idx]:\n",
    "    tdata[c] = (tdata[c] - tdata[c].min()) / (tdata[c].max() - tdata[c].min())\n",
    "tdata[tdata.columns[first_weather_col_idx:]] = (\n",
    "    tdata[tdata.columns[first_weather_col_idx:]]\n",
    "    - tdata[tdata.columns[first_weather_col_idx:]].mean()\n",
    ") / tdata[tdata.columns[first_weather_col_idx:]].std()\n",
    "\n",
    "# Create training and test rows\n",
    "split_idx = int((0.8 * len(tdata)) // 24 * 24)\n",
    "iso_str = f\"{tdata.index[split_idx]}\"[:10]\n",
    "cutoff_date = pd.to_datetime(iso_str)\n",
    "train_data = tdata.loc[(tdata.index < cutoff_date)]\n",
    "test_data = tdata.loc[(tdata.index >= cutoff_date)]\n",
    "# train_data = train_data.drop(\"daa_price_eurmwh\", axis=1)\n",
    "# test_data = test_data.drop(\"daa_price_eurmwh\", axis=1)\n",
    "train_rows = train_data.values.astype(\"float32\")\n",
    "test_rows = test_data.values.astype(\"float32\")\n",
    "price_idx = train_data.columns.tolist().index(\"idc_av_price_eurmwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8919c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/gxps7b2d6l3dps4vtczb2rh80000gn/T/ipykernel_63082/1157537686.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  return torch.tensor(X), torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and loaders\n",
    "\n",
    "from numpy import ndarray\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Datasets are created from sliding windows\n",
    "def create_dataset(\n",
    "    data: ndarray, lookback_hours: int, predict_hours: int\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    X, y = [], []\n",
    "    for i in range(lookback_hours, len(data) - predict_hours, 24):\n",
    "        feature = data[i - lookback_hours : i]\n",
    "        target = data[i : i + predict_hours][:, price_idx]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "# Create the sets\n",
    "lookback_hours = 1 * 24\n",
    "predict_hours = 1 * 24\n",
    "X_train, y_train = create_dataset(train_rows, lookback_hours, predict_hours)\n",
    "X_test, y_test = create_dataset(test_rows, lookback_hours, predict_hours)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe71d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size: int, num_layers=2, hidden_size=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Linear(hidden_size // 2, 8),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9ffc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Layers\tHidden Size\t# Params\tBlend Factor\tVal. Loss\tØ Diff DAA (EUR)\tσ Diff DAA (EUR)\tØ Diff Pred. (EUR)\tσ Diff Pred. (EUR)\tØ Diff Blended (EUR)\tσ Diff Blended (EUR)\n",
      "2\t64\t44209\t0.2\t0.2314\t12.781\t33.986\t20.378\t20.553\t11.837\t13.376\n",
      "2\t64\t44209\t0.2\t0.2315\t12.781\t33.986\t20.710\t19.726\t12.113\t13.410\n",
      "2\t64\t44209\t0.2\t0.2317\t12.781\t33.986\t20.581\t20.285\t11.960\t13.406\n",
      "2\t64\t44209\t0.2\t0.2296\t12.781\t33.986\t20.367\t20.060\t11.891\t13.284\n",
      "2\t64\t44209\t0.2\t0.2345\t12.781\t33.986\t21.196\t21.272\t12.019\t13.399\n",
      "3\t64\t69169\t0.2\t0.2349\t12.781\t33.986\t20.791\t21.271\t11.862\t13.472\n",
      "3\t64\t69169\t0.2\t0.2324\t12.781\t33.986\t20.246\t20.212\t11.929\t13.295\n",
      "3\t64\t69169\t0.2\t0.2375\t12.781\t33.986\t20.491\t20.611\t11.940\t13.404\n",
      "3\t64\t69169\t0.2\t0.2386\t12.781\t33.986\t21.062\t20.977\t11.890\t13.337\n",
      "3\t64\t69169\t0.2\t0.2447\t12.781\t33.986\t21.105\t20.865\t12.174\t13.449\n",
      "4\t64\t94129\t0.2\t0.2289\t12.781\t33.986\t20.700\t20.475\t12.098\t13.331\n",
      "4\t64\t94129\t0.2\t0.2364\t12.781\t33.986\t20.634\t20.884\t11.859\t13.320\n",
      "4\t64\t94129\t0.2\t0.2333\t12.781\t33.986\t20.547\t20.775\t11.972\t13.500\n",
      "4\t64\t94129\t0.2\t0.2348\t12.781\t33.986\t20.517\t20.436\t11.924\t13.377\n",
      "4\t64\t94129\t0.2\t0.2298\t12.781\t33.986\t21.200\t20.068\t12.086\t13.331\n",
      "2\t96\t95489\t0.2\t0.2281\t12.781\t33.986\t20.112\t20.441\t11.943\t13.479\n",
      "2\t96\t95489\t0.2\t0.2365\t12.781\t33.986\t21.244\t20.693\t11.963\t13.336\n",
      "2\t96\t95489\t0.2\t0.2363\t12.781\t33.986\t20.341\t21.010\t11.878\t13.391\n",
      "2\t96\t95489\t0.2\t0.2291\t12.781\t33.986\t20.075\t19.870\t11.953\t13.407\n",
      "2\t96\t95489\t0.2\t0.2329\t12.781\t33.986\t20.201\t20.548\t11.978\t13.377\n",
      "3\t96\t151361\t0.2\t0.2317\t12.781\t33.986\t21.511\t20.546\t12.238\t13.396\n",
      "3\t96\t151361\t0.2\t0.2318\t12.781\t33.986\t20.647\t21.130\t11.994\t13.434\n",
      "3\t96\t151361\t0.2\t0.2327\t12.781\t33.986\t19.924\t20.391\t11.832\t13.400\n",
      "3\t96\t151361\t0.2\t0.2376\t12.781\t33.986\t21.359\t20.792\t11.978\t13.354\n",
      "3\t96\t151361\t0.2\t0.2267\t12.781\t33.986\t20.614\t20.268\t11.960\t13.391\n",
      "4\t96\t207233\t0.2\t0.2329\t12.781\t33.986\t21.043\t20.631\t11.996\t13.425\n",
      "4\t96\t207233\t0.2\t0.2351\t12.781\t33.986\t20.509\t21.401\t11.854\t13.327\n",
      "4\t96\t207233\t0.2\t0.2388\t12.781\t33.986\t20.639\t21.177\t11.897\t13.384\n",
      "4\t96\t207233\t0.2\t0.2411\t12.781\t33.986\t20.385\t20.956\t11.932\t13.189\n",
      "4\t96\t207233\t0.2\t0.2325\t12.781\t33.986\t21.314\t21.976\t11.926\t13.387\n",
      "2\t128\t166225\t0.2\t0.2321\t12.781\t33.986\t21.131\t20.758\t11.795\t13.336\n",
      "2\t128\t166225\t0.2\t0.2315\t12.781\t33.986\t20.755\t21.056\t11.868\t13.519\n",
      "2\t128\t166225\t0.2\t0.2292\t12.781\t33.986\t20.208\t19.852\t11.993\t13.385\n",
      "2\t128\t166225\t0.2\t0.2336\t12.781\t33.986\t20.148\t20.550\t11.866\t13.450\n",
      "2\t128\t166225\t0.2\t0.2305\t12.781\t33.986\t21.377\t20.836\t12.013\t13.416\n",
      "3\t128\t265297\t0.2\t0.2300\t12.781\t33.986\t22.059\t21.397\t12.160\t13.347\n",
      "3\t128\t265297\t0.2\t0.2327\t12.781\t33.986\t20.861\t20.875\t12.064\t13.360\n",
      "3\t128\t265297\t0.2\t0.2297\t12.781\t33.986\t21.175\t19.990\t12.086\t13.290\n",
      "3\t128\t265297\t0.2\t0.2339\t12.781\t33.986\t20.803\t20.679\t12.017\t13.358\n",
      "3\t128\t265297\t0.2\t0.2343\t12.781\t33.986\t20.635\t21.032\t11.983\t13.370\n",
      "4\t128\t364369\t0.2\t0.2438\t12.781\t33.986\t20.915\t21.793\t11.934\t13.461\n",
      "4\t128\t364369\t0.2\t0.2389\t12.781\t33.986\t20.755\t20.791\t11.937\t13.355\n",
      "4\t128\t364369\t0.2\t0.2350\t12.781\t33.986\t20.807\t20.416\t12.034\t13.394\n",
      "4\t128\t364369\t0.2\t0.2277\t12.781\t33.986\t21.075\t21.388\t11.998\t13.320\n",
      "4\t128\t364369\t0.2\t0.2320\t12.781\t33.986\t20.997\t20.828\t12.094\t13.324\n",
      "2\t192\t366065\t0.2\t0.2347\t12.781\t33.986\t21.275\t20.572\t11.926\t13.375\n",
      "2\t192\t366065\t0.2\t0.2309\t12.781\t33.986\t20.763\t21.186\t11.841\t13.464\n",
      "2\t192\t366065\t0.2\t0.2378\t12.781\t33.986\t21.723\t21.912\t12.030\t13.318\n",
      "2\t192\t366065\t0.2\t0.2339\t12.781\t33.986\t21.193\t21.388\t11.998\t13.362\n",
      "2\t192\t366065\t0.2\t0.2295\t12.781\t33.986\t20.710\t20.641\t12.102\t13.385\n",
      "3\t192\t588401\t0.2\t0.2466\t12.781\t33.986\t21.673\t20.409\t12.261\t13.322\n",
      "3\t192\t588401\t0.2\t0.2283\t12.781\t33.986\t20.676\t20.225\t11.906\t13.340\n",
      "3\t192\t588401\t0.2\t0.2348\t12.781\t33.986\t21.477\t21.645\t12.009\t13.427\n",
      "3\t192\t588401\t0.2\t0.2344\t12.781\t33.986\t20.416\t20.545\t11.906\t13.317\n",
      "3\t192\t588401\t0.2\t0.2322\t12.781\t33.986\t21.413\t20.371\t12.065\t13.346\n",
      "4\t192\t810737\t0.2\t0.2486\t12.781\t33.986\t21.285\t22.371\t11.906\t13.461\n",
      "4\t192\t810737\t0.2\t0.2387\t12.781\t33.986\t21.273\t21.300\t12.173\t13.390\n",
      "4\t192\t810737\t0.2\t0.2357\t12.781\t33.986\t20.649\t20.922\t11.949\t13.453\n",
      "4\t192\t810737\t0.2\t0.2391\t12.781\t33.986\t21.000\t21.575\t11.967\t13.312\n",
      "4\t192\t810737\t0.2\t0.2329\t12.781\t33.986\t20.682\t20.763\t11.919\t13.326\n",
      "2\t256\t643729\t0.2\t0.2344\t12.781\t33.986\t21.503\t20.929\t11.917\t13.334\n",
      "2\t256\t643729\t0.2\t0.2332\t12.781\t33.986\t22.582\t22.538\t12.381\t13.331\n",
      "2\t256\t643729\t0.2\t0.2375\t12.781\t33.986\t21.405\t21.158\t11.858\t13.427\n",
      "2\t256\t643729\t0.2\t0.2393\t12.781\t33.986\t21.650\t21.555\t12.059\t13.331\n",
      "2\t256\t643729\t0.2\t0.2431\t12.781\t33.986\t20.706\t20.928\t11.923\t13.295\n",
      "3\t256\t1038481\t0.2\t0.2410\t12.781\t33.986\t21.651\t20.523\t12.101\t13.309\n",
      "3\t256\t1038481\t0.2\t0.2354\t12.781\t33.986\t20.394\t20.519\t12.062\t13.423\n",
      "3\t256\t1038481\t0.2\t0.2443\t12.781\t33.986\t21.217\t20.802\t12.093\t13.073\n",
      "3\t256\t1038481\t0.2\t0.2346\t12.781\t33.986\t20.153\t20.555\t11.946\t13.437\n",
      "3\t256\t1038481\t0.2\t0.2361\t12.781\t33.986\t22.767\t22.446\t12.306\t13.462\n",
      "4\t256\t1433233\t0.2\t0.2436\t12.781\t33.986\t22.186\t22.742\t12.024\t13.395\n",
      "4\t256\t1433233\t0.2\t0.2382\t12.781\t33.986\t20.466\t20.911\t11.929\t13.427\n",
      "4\t256\t1433233\t0.2\t0.2383\t12.781\t33.986\t20.635\t21.565\t11.879\t13.303\n",
      "4\t256\t1433233\t0.2\t0.2396\t12.781\t33.986\t20.701\t22.057\t11.928\t13.381\n",
      "4\t256\t1433233\t0.2\t0.2411\t12.781\t33.986\t21.656\t21.031\t11.804\t13.507\n",
      "2\t384\t1432529\t0.2\t0.2416\t12.781\t33.986\t22.328\t22.429\t12.277\t13.360\n",
      "2\t384\t1432529\t0.2\t0.2452\t12.781\t33.986\t22.450\t22.646\t12.161\t13.280\n",
      "2\t384\t1432529\t0.2\t0.2397\t12.781\t33.986\t20.814\t20.961\t12.174\t13.392\n",
      "2\t384\t1432529\t0.2\t0.2361\t12.781\t33.986\t21.241\t21.097\t12.050\t13.453\n",
      "2\t384\t1432529\t0.2\t0.2439\t12.781\t33.986\t21.718\t21.592\t12.083\t13.303\n",
      "3\t384\t2319569\t0.2\t0.2370\t12.781\t33.986\t21.192\t21.916\t12.132\t13.407\n",
      "3\t384\t2319569\t0.2\t0.2576\t12.781\t33.986\t21.002\t21.764\t11.943\t13.340\n",
      "3\t384\t2319569\t0.2\t0.2443\t12.781\t33.986\t21.655\t22.091\t12.157\t13.389\n",
      "3\t384\t2319569\t0.2\t0.2423\t12.781\t33.986\t21.606\t22.081\t12.085\t13.359\n",
      "3\t384\t2319569\t0.2\t0.2381\t12.781\t33.986\t23.416\t22.578\t12.397\t13.480\n",
      "4\t384\t3206609\t0.2\t0.2487\t12.781\t33.986\t21.792\t21.124\t12.135\t13.335\n",
      "4\t384\t3206609\t0.2\t0.2436\t12.781\t33.986\t21.449\t21.107\t12.102\t13.365\n",
      "4\t384\t3206609\t0.2\t0.2483\t12.781\t33.986\t21.325\t21.232\t12.145\t13.364\n",
      "4\t384\t3206609\t0.2\t0.2407\t12.781\t33.986\t21.077\t21.563\t11.956\t13.523\n",
      "4\t384\t3206609\t0.2\t0.2445\t12.781\t33.986\t21.336\t21.174\t12.061\t13.428\n"
     ]
    }
   ],
   "source": [
    "# Iterate hyperparams, train and evaluate the model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "max_epochs = 200\n",
    "patience = 10\n",
    "blend_fac = 0.2\n",
    "\n",
    "# Print the header\n",
    "print(\n",
    "    \"# Layers\\tHidden Size\\t# Params\\tBlend Factor\\tVal. Loss\\tØ Diff DAA (EUR)\\tσ Diff DAA (EUR)\\tØ Diff Pred. (EUR)\\tσ Diff Pred. (EUR)\\tØ Diff Blended (EUR)\\tσ Diff Blended (EUR)\"\n",
    ")\n",
    "\n",
    "for hidden_size in [64, 96, 128, 192, 256, 384]:\n",
    "    for num_layers in [2, 3, 4]:\n",
    "        for run in range(5):\n",
    "            # Create the model\n",
    "            loss_fn = nn.MSELoss()\n",
    "            model = GRUModel(X_train.shape[2], num_layers, hidden_size).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"min\", factor=0.5, patience=5\n",
    "            )\n",
    "            trainable_params = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            )\n",
    "\n",
    "            # Train the model until there's no more improvement\n",
    "            best_val_loss = float(\"inf\")\n",
    "            best_weights = None\n",
    "            patience_counter = 0\n",
    "            for epoch in range(max_epochs):\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(X_batch)\n",
    "                    loss = loss_fn(output, torch.unsqueeze(y_batch, 2))\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                # Calculate validation loss\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in test_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        output = model(torch.squeeze(X_batch))\n",
    "                        val_loss += loss_fn(output, torch.unsqueeze(y_batch, 2)).item()\n",
    "                val_loss /= len(test_loader)\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "                # Stop early if there's no more improvement\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_weights = model.state_dict()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        break\n",
    "\n",
    "            # Evaluate the price difference\n",
    "            with torch.no_grad():\n",
    "                model.cpu()\n",
    "                # Get the predicted data\n",
    "                y_pred_test = model(X_test).clone().detach().cpu().numpy()\n",
    "                y_pred_test = y_pred_test.reshape(\n",
    "                    y_pred_test.shape[0] * y_pred_test.shape[1]\n",
    "                )\n",
    "                test_plot = np.ones_like(tdata[\"idc_av_price_eurmwh\"]) * np.nan\n",
    "                test_plot[\n",
    "                    len(train_rows) + lookback_hours : len(train_rows)\n",
    "                    + lookback_hours\n",
    "                    + y_pred_test.shape[0]\n",
    "                ] = y_pred_test\n",
    "\n",
    "                # Transform it back\n",
    "                real_price = (\n",
    "                    tdata[\"idc_av_price_eurmwh\"].to_numpy() * price_std + price_mean\n",
    "                )\n",
    "                daa_price = (\n",
    "                    tdata[\"daa_price_eurmwh\"].to_numpy() * price_std + price_mean\n",
    "                )\n",
    "                pred_test_price = test_plot * price_std + price_mean\n",
    "\n",
    "                # Calculate the blended price\n",
    "                blend_price = (1 - blend_fac) * daa_price + blend_fac * pred_test_price\n",
    "\n",
    "                # Calculate the absolute diff meand and stddev\n",
    "                pdata = pd.DataFrame({\"intraday_price\": real_price}, index=tdata.index)\n",
    "                daa_diff = pdata[\"intraday_price\"] - daa_price\n",
    "                daa_adm = daa_diff.abs().mean()\n",
    "                daa_ads = daa_diff.abs().std()\n",
    "                pred_diff = pdata[\"intraday_price\"] - pred_test_price\n",
    "                pred_adm = pred_diff.abs().mean()\n",
    "                pred_ads = pred_diff.abs().std()\n",
    "                blend_diff = pdata[\"intraday_price\"] - blend_price\n",
    "                blend_adm = blend_diff.abs().mean()\n",
    "                blend_ads = blend_diff.abs().std()\n",
    "\n",
    "                # Print the result\n",
    "                print(\n",
    "                    f\"{num_layers}\\t{hidden_size}\\t{trainable_params}\\t{blend_fac}\\t{best_val_loss:.4f}\\t{daa_adm:.3f}\\t{daa_ads:.3f}\\t{pred_adm:.3f}\\t{pred_ads:.3f}\\t{blend_adm:.3f}\\t{blend_ads:.3f}\"\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
